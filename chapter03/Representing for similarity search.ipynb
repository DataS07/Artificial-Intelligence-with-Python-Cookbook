{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyQuYILRn7RS"
   },
   "outputs": [],
   "source": [
    "# this is the dataset to use\n",
    "# https://github.com/ofrendo/WebDataIntegration/blob/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "8WYsEdpDPKU4",
    "outputId": "c8d29ad4-69ee-4fac-f277-152e4a08afec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-30 16:15:53--  https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7324 (7.2K) [text/plain]\n",
      "Saving to: ‘forbes_freebase_goldstandard_train.csv’\n",
      "\n",
      "\r",
      "          forbes_fr   0%[                    ]       0  --.-KB/s               \r",
      "forbes_freebase_gol 100%[===================>]   7.15K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-01-30 16:15:53 (213 MB/s) - ‘forbes_freebase_goldstandard_train.csv’ saved [7324/7324]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI67lN68POnO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('forbes_freebase_goldstandard_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO_s51GFQHy1"
   },
   "source": [
    "loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "aXGqoY_9QBj9",
    "outputId": "d35bd9ff-e7ca-4f9a-e6c8-84b076304304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-30 16:19:47--  https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 976 [text/plain]\n",
      "Saving to: ‘forbes_freebase_goldstandard_test.csv’\n",
      "\n",
      "\r",
      "          forbes_fr   0%[                    ]       0  --.-KB/s               \r",
      "forbes_freebase_gol 100%[===================>]     976  --.-KB/s    in 0s      \n",
      "\n",
      "2020-01-30 16:19:47 (200 MB/s) - ‘forbes_freebase_goldstandard_test.csv’ saved [976/976]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "s1B_H9B0PVVh",
    "outputId": "1cc7d96a-3614-427c-eb95-2de050520b01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>General Electric</th>\n",
       "      <th>General Electric.1</th>\n",
       "      <th>TRUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank of China</td>\n",
       "      <td>Industrial and Commercial Bank of China (Asia)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetroChina</td>\n",
       "      <td>PetroChina</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citigroup</td>\n",
       "      <td>Citigroup</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  General Electric                              General Electric.1  TRUE\n",
       "0      Wells Fargo                                     Wells Fargo  True\n",
       "1    Bank of China  Industrial and Commercial Bank of China (Asia)  True\n",
       "2       PetroChina                                      PetroChina  True\n",
       "3            Apple                                      Apple Inc.  True\n",
       "4        Citigroup                                       Citigroup  True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AaWETqtZPach",
    "outputId": "827e61ef-f4bd-4ae1-d64f-850b5f5a3ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "JHkt36DhwkUt",
    "outputId": "5b44ed47-4a50-403b-fb66-a05a28336ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\r",
      "\u001b[K     |██████▊                         | 10kB 13.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 20kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 30kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 40kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (42.0.2)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144668 sha256=1ec94057e46fbb71326db91a3e18b68e799ebb8b36e245ac6b79597763043eb8\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "211FA9izvDG2"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from difflib import SequenceMatcher  # for longest common substring\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import Levenshtein  # levenstein/edit distance; docs here: https://rawgit.com/ztane/python-Levenshtein/master/docs/Levenshtein.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Sif6SdJpDNb"
   },
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    '''We will use this functions to remove special characters etc before \n",
    "    any string distance calculation.\n",
    "    '''\n",
    "    return ''.join(map(lambda x: x.lower() if str.isalnum(x) else ' ', string)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBIyzgvhvI1z"
   },
   "source": [
    "I think it's useful to make a distinction here\n",
    "- distance functions between two strings\n",
    "- string featurization\n",
    "\n",
    "The string distances we can use to establish a baseline performance. But with small changes we can make them string featurization functions and use them in classifier functions in a machine learning approach. However, let us get first to string distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8logLPvLve7y"
   },
   "outputs": [],
   "source": [
    "def levenstein_distance(s1_, s2_):\n",
    "    s1, s2 = clean_string(s1_), clean_string(s2_)\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    return Levenshtein.distance(\n",
    "        s1, s2\n",
    "    ) / max([len_s1, len_s2])\n",
    "\n",
    "def jaro_winkler_distance(s1_, s2_):\n",
    "    s1, s2 = clean_string(s1_), clean_string(s2_)\n",
    "    return Levenshtein.jaro_winkler(s1, s2)\n",
    "\n",
    "def common_substring_similarity(s1_, s2_):\n",
    "    s1, s2 = clean_string(s1_), clean_string(s2_)\n",
    "    match = SequenceMatcher(None, s1, s2).find_longest_match(0, len_s1, 0, len_s2)\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    norm = max([len_s1, len_s2])\n",
    "    return min([1, match.size / norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBO1cvFZn6-Z"
   },
   "outputs": [],
   "source": [
    "# we can use these string functions for fuzzy string match\n",
    "# some matches are not very good, so we should make sure, we have thresholds.\n",
    "\n",
    "pool = Pool(50)\n",
    "\n",
    "def fuzzy_string_search(\n",
    "    s1, string_list,\n",
    "     string_compare,\n",
    "      threshold=lambda sim: sim>0.7\n",
    "    ):\n",
    "    '''Search through a list of strings using a string_comparison function\n",
    "    in order to find the best match.\n",
    "\n",
    "    Parameters:\n",
    "    - s1: string to search for\n",
    "    - string_list: list of strings\n",
    "    - string_compare: string comparison function to return a similarity or a \n",
    "      distance.\n",
    "    - threshold: cut-off function to decide if returning the best match or\n",
    "      nothing at all. This threshold function has to take into account if we\n",
    "      are using a string similarity or a string distance.\n",
    "\n",
    "    Return the best matching string if threshold reached.\n",
    "    Otherwise return None.\n",
    "\n",
    "    Example:\n",
    "    >> company_list = [\n",
    "        'Blackrock',\n",
    "        'Credit Suisse',\n",
    "        'Goldman Sachs',\n",
    "        'Bank of America/Meryll Lynch',\n",
    "        'Morgan Stanley',\n",
    "        'LEK',\n",
    "        'JP Morgan',\n",
    "        'Nomura',\n",
    "        'BNP Paribas',\n",
    "        'WPP',\n",
    "        'Rothschild',\n",
    "        'Allianz',\n",
    "    ]\n",
    "    >> fuzzy_string_search(\n",
    "      'SAP',\n",
    "      company_list,\n",
    "      jaro_winkler_distance,\n",
    "      threshold=lambda dist: dist<0.7\n",
    "    )\n",
    "    '''\n",
    "    string_match = partial(string_compare, s2_=s1)\n",
    "    comparisons = pool.map(string_match, string_list)\n",
    "    index, element = max(enumerate(comparisons), key=itemgetter(1))\n",
    "    #print(f\"match {string_list[index]} with score {element}\")\n",
    "    if threshold(element):\n",
    "        return string_list[index]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HbwJKVc3yZrJ",
    "outputId": "eaee3474-71a2-4c8b-d62a-b79d73c8f27c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WPP'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_list = [\n",
    "        'Blackrock',\n",
    "        'Credit Suisse',\n",
    "        'Goldman Sachs',\n",
    "        'Bank of America/Meryll Lynch',\n",
    "        'Morgan Stanley',\n",
    "        'LEK',\n",
    "        'JP Morgan',\n",
    "        'Nomura',\n",
    "        'BNP Paribas',\n",
    "        'WPP',\n",
    "        'Rothschild',\n",
    "        'Allianz',\n",
    "    ]\n",
    "fuzzy_string_search(\n",
    "      'SAP',\n",
    "      company_list,\n",
    "      jaro_winkler_distance,\n",
    "      threshold=lambda dist: dist<0.7\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zvf4CavVtWmT"
   },
   "outputs": [],
   "source": [
    "# We can also use featurizations of strings, for example using sklearn\n",
    "# inbuilt functionality such as CountVectorizers or TfidfVectorizer.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# the CountVectorizer counts the occurences of features. These features\n",
    "# can be composed of characters or words; we are interested in character-\n",
    "# based features. We clean the strings as before and we take ngrams.\n",
    "# Also try TfidfVectorizer for a baseline performance\n",
    "ngram_featurizer = CountVectorizer(\n",
    "    min_df=1,\n",
    "    analyzer='char',\n",
    "    ngram_range=(1,1),  # this is the range of ngrams that are to be extracted!\n",
    "    preprocessor=clean_string\n",
    ")\n",
    "company_space = ngram_featurizer.fit_transform(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "68Kdi7sWyej3",
    "outputId": "5acba677-1112-4b81-8f7e-b7ab87eeeb44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 96 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eT2hm7z2D2Z"
   },
   "outputs": [],
   "source": [
    "# Some alternative way to featurize strings. These can be used in similar ways \n",
    "# to the CountVectorizer really. Apply these as a preprocessor to a classifier\n",
    "# and check the performance in distinguishing between match and no-match.\n",
    "\n",
    "def editops_featurizer(s1_, s2_):\n",
    "    '''Counts the replace, insert and delete operations between two strings\n",
    "    and normalizes these by maximum string length.\n",
    "\n",
    "    This featurization could be interesting to find out which operation is\n",
    "    most useful.\n",
    "    '''\n",
    "    s1, s2 = clean_string(s1_), clean_string(s2_)\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    ops = Levenshtein.editops(\n",
    "        s1, s2\n",
    "    )\n",
    "    index_dict = {'insert': 0, 'replace': 1, 'delete': 2}\n",
    "    features = np.zeros((3))\n",
    "    for op in edit_ops:\n",
    "        features[index_dict[op[0]]] += 1\n",
    "    features / max([len_s1, len_s2])  \n",
    "    return features\n",
    "\n",
    "def common_substring_featurizer(s1_, s2_):\n",
    "    '''Here we extract 1. the normalized length of the common substring\n",
    "    and whether the common substring matches 2. the beginning or\n",
    "    3. the end of a word.\n",
    "    '''\n",
    "    s1, s2 = clean_string(s1_), clean_string(s2_)\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    longer_string = s1 if len_s1 > len_s2 else s2\n",
    "    norm = max([len_s1, len_s2])    \n",
    "    match = SequenceMatcher(None, s1, s2).find_longest_match(0, len_s1, 0, len_s2)\n",
    "    substring = s1[match.a: match.a + match.size]\n",
    "    \n",
    "    m1 = re.search(\n",
    "        '(?:^|\\s|[a-z])' + substring,\n",
    "        longer_string\n",
    "    )\n",
    "    m2 = re.search(\n",
    "        substring + '(?:[a-z]|\\s|$)',\n",
    "        longer_string\n",
    "    )\n",
    "    return min([1, match.size / norm]), m1 is not None, m2 is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "colab_type": "code",
    "id": "e0T2OgbZp8jt",
    "outputId": "f3baa8c6-7d14-419b-fbd4-b7c19af7288f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                1250      \n",
      "=================================================================\n",
      "Total params: 1,250\n",
      "Trainable params: 1,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 models to be trained against each other\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 50)           1250        input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,250\n",
      "Trainable params: 1,250\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# siamese network dimensionality reduction\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Lambda, Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def create_string_featurization_model(feature_dimensionality, output_dim=50):\n",
    "    '''\n",
    "    Use for string featurization in combination with siamese models.\n",
    "    Just a non-linear projection as a way of reducing the feature dimensionality\n",
    "    in a meaningful way.\n",
    "\n",
    "    Parameters:\n",
    "        feature_dimensionality - number of features coming from the vectorizer\n",
    "          or string featurization function\n",
    "        output_dim - dimensions of the embedding/projection that we are trying\n",
    "          to create\n",
    "    '''\n",
    "    preprocessing_model = Sequential()\n",
    "    preprocessing_model.add(\n",
    "        Dense(output_dim, activation='selu', input_dim=feature_dimensionality)\n",
    "    )\n",
    "    preprocessing_model.summary()\n",
    "    return preprocessing_model\n",
    "\n",
    "def create_siamese_model(preprocessing_models, #initial_bias =\n",
    "                          input_shapes=(10,)):\n",
    "    def euclidean_distance(vects):\n",
    "        x, y = vects\n",
    "        x = K.l2_normalize(x, axis=-1)\n",
    "        y = K.l2_normalize(y, axis=-1)\n",
    "        sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "        return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "    \n",
    "    if not isinstance(preprocessing_models, (list, tuple)):\n",
    "        raise ValueError('preprocessing models needs to be a list or tuple of models')\n",
    "\n",
    "    print('{} models to be trained against each other'.format(len(preprocessing_models)))\n",
    "    if not isinstance(input_shapes, list):\n",
    "        input_shapes = [input_shapes] * len(preprocessing_models)\n",
    "    \n",
    "    inputs = []\n",
    "    intermediate_layers = []\n",
    "    for preprocessing_model, input_shape in zip(preprocessing_models, input_shapes):\n",
    "        inputs.append(Input(shape=input_shape))\n",
    "        intermediate_layers.append(preprocessing_model(inputs[-1]))\n",
    "\n",
    "    layer_diffs = []\n",
    "    for i in range(len(intermediate_layers)-1):        \n",
    "        layer_diffs.append(\n",
    "            Lambda(euclidean_distance)([intermediate_layers[i], intermediate_layers[i+1]])\n",
    "        )    \n",
    "    siamese_model = Model(inputs=inputs, outputs=layer_diffs)\n",
    "    siamese_model.summary()\n",
    "    return siamese_model\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='mse',\n",
    "        metrics=[\n",
    "            #'accuracy',\n",
    "            #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "            #tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            #tf.keras.metrics.AUC(name='auc'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# use like this:\n",
    "feature_dims = len(ngram_featurizer.get_feature_names())\n",
    "string_featurization_model = create_string_featurization_model(feature_dims, output_dim=50)\n",
    "\n",
    "siamese_model = create_siamese_model(\n",
    "    preprocessing_models=[string_featurization_model, string_featurization_model],\n",
    "    input_shapes=[(feature_dims,), (feature_dims,)],\n",
    ")\n",
    "compile_model(siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "t6lo6MIN9QwY",
    "outputId": "be86cd9a-e17a-4d75-97c8-a867335fbc24"
   },
   "outputs": [],
   "source": [
    "siamese_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dj5u2rKxq5jc"
   },
   "outputs": [],
   "source": [
    "# some very basic plotting stuff\n",
    "# maybe useful for training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_metrics(history, metrics=['loss', 'auc', 'precision', 'recall']):\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        \n",
    "    mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "        plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "        plt.ylim([0.8,1])\n",
    "    else:\n",
    "        plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "#plot_metrics(history, metrics=['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fq2qugA76a9o",
    "outputId": "72845959-0ecd-41ba-9981-cccf87bab029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/66/eab272ae940d36d698994058e303fe7d1264d10ec120e0a508d0c8fb3ca5/annoy-1.16.2.tar.gz (636kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 23.0MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 245kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 266kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 276kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 286kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 296kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 307kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 317kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 327kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 337kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 348kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 358kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 368kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 378kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 389kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 399kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 409kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 419kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 430kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 440kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 450kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 460kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 471kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 481kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 491kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 501kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 512kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 522kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 532kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 542kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 552kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 563kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 573kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 583kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 593kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 604kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 614kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 624kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 634kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for annoy: filename=annoy-1.16.2-cp36-cp36m-linux_x86_64.whl size=310418 sha256=9c0c67d0ed535901d016f5cbd0dfe87ce50949631eb69f250acfd494359b0d23\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/d7/68/3795670ef4c6781fc10df0d6cf83b922244aa28cd9489d1176\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy  # https://github.com/spotify/annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "9ozdnuxFprNx",
    "outputId": "4c99ff05-d776-4cd1-ff67-cca7be7a52f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given featurized strings, we can use nearest neighbor-search/classification\n",
    "# to decide very quickly which string matches and how good it matches.\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "index = AnnoyIndex(company_space.shape[1], 'euclidean')\n",
    "for i, emp in enumerate(company_space.toarray()):\n",
    "    index.add_item(i, emp)\n",
    "    \n",
    "index.build(10)\n",
    "index.save('string_list.ann')\n",
    "\n",
    "\n",
    "v = ngram_featurizer.transform(['Meryll Lynch']).toarray().flatten()\n",
    "\n",
    "ngram_featurizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3EmNdji7JI2Z",
    "outputId": "890cc39b-9f52-40a4-f16c-f635b2a38ee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.get_nns_by_vector(v, 3, search_k=-1, include_distances=True)\n",
    "ngram_featurizer.transform(['HSBC']).toarray()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Finding the best fuzzy string matching",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
